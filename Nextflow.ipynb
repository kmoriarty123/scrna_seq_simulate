{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb4a3cb8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Nextflow](#Nextflow)\n",
    "    * [Tutorial](#Running-the-nextflow-tutorial)\n",
    "    * [Requirements](#Requirements)\n",
    "    * [script1.nf](#script1.nf)\n",
    "    * [script2.nf](#script2.nf)\n",
    "    * [script3.nf](#script3.nf)\n",
    "    * [script4.nf](#script4.nf)\n",
    "    * [script5.nf](#script5.nf)\n",
    "    * [script6.nf](#script6.nf)\n",
    "    * [script7.nf](#script7.nf)\n",
    "    * [Own Scripts](#Using-custom-scripts)\n",
    "    * [Executors](#Managing-execution-with-executors)\n",
    "    * [Modularization](#Workflow-modularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e44b7",
   "metadata": {},
   "source": [
    "# Nextflow\n",
    "\n",
    "`Nextflow` is a workflow programming language, based on `Groovy`, based on `Java`. Therefore, it supports general programming as well as has specific constructs for specifying and running workflows. A introductory course on the language can be found at https://carpentries-incubator.github.io/workflows-nextflow/aio/index.html. In addition, the developers have prepared a tutorial which I have commented below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b3d31c",
   "metadata": {},
   "source": [
    "# Running the nextflow tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f6d838",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Tutorial pulled from github\n",
    "\n",
    "https://github.com/seqeralabs/nextflow-tutorial\n",
    "\n",
    "### Docker \n",
    "Docker is required for running the workflow, as it uses images of various software such as `salmon`. Docker can be downloaded from Dockerhub and installed in Applications. Then a docker daemon, necessary for pulling images from Dockerhub, can be started by double-clicking on the Docker application.\n",
    "\n",
    "### Nextflow scripting\n",
    "\n",
    "**Note:** nextflow uses the `groovy` programming language (http://groovy-lang.org/documentation.html), which is based on `Java`.\n",
    "\n",
    "Info on nextflow scripting can be found https://www.nextflow.io/docs/latest/script.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cb6ae1",
   "metadata": {},
   "source": [
    "## script1.nf\n",
    "\n",
    "Looks like this:\n",
    "\n",
    "```\n",
    "/*                                                                                                                     \n",
    "* pipeline input parameters                                                                                           \n",
    "*/\n",
    "\n",
    "params.reads = \"$baseDir/data/ggal/gut_{1,2}.fq\"                                                                       \n",
    "params.transcript = \"$baseDir/data/ggal/transcriptome.fa\"                                                              \n",
    "params.multiqc = \"$baseDir/multiqc\"                                                                                    \n",
    "params.outdir = \"$baseDir/output\"                                                                                      \n",
    "                                                                                                                       \n",
    "/* println \"reads: $params.reads\" */                                                                                   \n",
    "                                                                                                                       \n",
    "log.info \"\"\"\\                                                                                                          \n",
    " R N A S E Q - N F   P I P E L I N E                                                                                   \n",
    " ===================================                                                                                   \n",
    " transcriptome: ${params.transcript}                                                                                   \n",
    " reads        : ${params.reads}                                                                                        \n",
    " outdir       : ${params.outdir}                                                                                       \n",
    " \"\"\"                                                                                                                   \n",
    "```\n",
    "Parameters are defined as part of a *params* structure (which is a name scope), thus params. will be the common prefix of these parameters. **Note:** variable interpolation is allowed **within double-quoted strings**. That is, although both single and double quotes can be used to denote strings, variable interpolation is only possible with the latter.\n",
    "\n",
    "`$baseDir` seems to be the directory from which the script is run. It is only one of the implicit variables defined in the global scope of the script, the list including:\n",
    "\n",
    "* baseDir - directory where the main workflow script is located (deprecated in favor of projectDir).\n",
    "* launchDir - directory where the workflow is run (requires version 20.04.0 or later).\n",
    "* moduleDir - directory where a module script is located for DSL2 modules or the same as projectDir for a non-module script (requires version 20.04.0 or later).\n",
    "* nextflow - dictionary-like object representing nextflow runtime information (have to check Nextflow metadata).\n",
    "* params - dictionary-like object holding workflow parameters specified in the config file or as commandline options.\n",
    "* projectDir - directory where the main script is located (requires version 20.04.0 or later).\n",
    "* workDir - directory where tasks temporary files are created.\n",
    "* workflow - dictionary-like object representing workflow runtime information (have to check Runtime metadata).\n",
    "\n",
    "Once a parameter is defined, it can be used by name in the nextflow script, and it can be dereferenced within string variables as `${varname}`.\n",
    "\n",
    "Comments have **C**-type syntax.\n",
    "\n",
    "Two types of print commands were used in the script, `println` (print line) and `log.info`, which is used to print multi-line strings to standard output. The syntax uses triple quotes, `\"\"\"`, like in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcf37d",
   "metadata": {},
   "source": [
    "## script2.nf\n",
    "\n",
    "Builds onto the first script, adding the following lines:\n",
    "\n",
    "```\n",
    "/*                                                                                                                     \n",
    " * define the `index` process that create a binary index                                                               \n",
    " * given the transcriptome file                                                                                       \n",
    " */                                                                                                                    \n",
    "process index {                                                                                                                                                                                                                             \n",
    "    input:                                                                                                             \n",
    "    path transcriptome from params.transcript                                                                         \n",
    "                                                                                                                       \n",
    "    output:                                                                                                           \n",
    "    path 'index' into index_ch                                                                                         \n",
    "                                                                                                                       \n",
    "    script:                                                                                                           \n",
    "    \"\"\"                                                                                                               \n",
    "    salmon index --threads $task.cpus -t $transcriptome -i index                                                       \n",
    "    \"\"\"                                                                                                                                                                                                                                     \n",
    "}                                                                                                                                                                                                                                           \n",
    "index_ch.view { \"Index found it $it\" }                                                                                 \n",
    "\n",
    "```\n",
    "\n",
    "It shows how a `process` is defined, essentially as a block of code in Python, delimited by `{}`. Within the *process*, there are sections introduced by keywords, specifically\n",
    "* input\n",
    "* output\n",
    "* script\n",
    "\n",
    "#### Syntax of sections in the process definition:\n",
    "```\n",
    "input:\n",
    "  <input qualifier> <input name> [from <source channel>] [attributes]\n",
    "``` \n",
    "The qualifier specifies the type of data to be received, and can take the following values:\n",
    "* val - allows the received input value to be accessed by its name in the process script, e.g. val x from num, where num is a channel\n",
    "* env - define an environment variable in the process execution context, based on the value received from the channel.\n",
    "* file - handle the received value as a file. If the same file name is to be used, irrespective of the values that come from the input channel, this name can be specified as follows `file query_file_name 'query.fa' from proteins` or `file 'query.fa' from proteins`. In this case, the execution is done in a separate environment, as if the files in the input channel were copied over into a file whose name does not change.\n",
    "* path - interprets string values as the path location of the input file and automatically converts to a file object; more recently introduced than file\n",
    "* stdin - forward the received value to stdin.\n",
    "* tuple - allows grouping multiple parameters in a single parameter definition. The items can be of the following types: val(x), file(x), file(‘name’), file(x:’name’), stdin, env(x). This allows the values of the respective variables to be set based on the inputs from the channel.\n",
    "* each - allows looping over items in a collection\n",
    "\n",
    "**Example `input` section:**\n",
    "```\n",
    "path transcriptome from params.transcript\n",
    "```\n",
    "following the definition of the `params.transcript` variable:\n",
    "```\n",
    "params.transcript = \"$baseDir/data/ggal/transcriptome.fa\"\n",
    "```\n",
    "So the syntax is similar to a `foreach i in set_of_i's` in python, the variable *transcriptome* takes on the values from the *params.transcript* channel.  \n",
    "\n",
    "**Example `output` section:**\n",
    "```\n",
    "path 'index' into index_ch\n",
    "```\n",
    "The *index_ch* channel is linked to a file called 'index', probably placed in the working directory. If the input channel had multiple files, the index file would probably be rewitten.\n",
    "\n",
    "**Example script section** (in triple quotes):\n",
    "```\n",
    "salmon index --threads \\\\$task.cpus -t \\\\$transcriptome -i index \n",
    "``` \n",
    "\n",
    "Should run `salmon`, although from the script it is not clear where it gets it from. It is probably included in the docker image associated with this tutorial. First occurrence of *index* is the option to salmon indicating what function needs to be run. `$task.cpus` gives the value of the `cpus` directive for the current task. It's an interpolated variable defined implicitly by nextflow. `$transcriptome` is also interpolated from the input definition above, and the second occurrence of *index* is a file name, the same that occurs from the output definition above. Single quotes do not allow variable interpolation, while double quotes do (I presume triple quotes, like here, also allow for interpolation). So the script section provides the exact commandline that needs to be run.\n",
    "\n",
    "#### Directives\n",
    "\n",
    "`cpus` is an example of a proces *directive*. Directives have to be speciefied before any other blocks (like input, output etc.) and without using the assignment operator <kbd>=</kbd>. Example of using directives in the scope of a project:\n",
    "```\n",
    "process big_job {\n",
    "\n",
    "  cpus 8\n",
    "  executor 'sge'\n",
    "\n",
    "  \"\"\"\n",
    "  blastp -query input_sequence -num_threads ${task.cpus}\n",
    "  \"\"\"\n",
    "}\n",
    "```\n",
    "Available directives are:\n",
    "* accelerator - e.g. \"accelerator 4, type: 'nvidia-tesla-k80'\"\n",
    "* afterScript \n",
    "* beforeScript - e.g. \"beforeScript 'source /cluster/bin/setup'\"\n",
    "* cache - allows caching of results \n",
    "* cpus - number of (logical) CPU required by the process’ task\n",
    "* conda -  definition of the process dependencies using Conda, multiple can be specified separated by spaces, syntax can be conda channel:package\n",
    "* container - allows to execute the process script in a Docker container if the host machine has a Docker daemon running.\n",
    "* containerOptions\n",
    "* clusterOptions\n",
    "* disk\n",
    "* echo\n",
    "* errorStrategy\n",
    "* executor - default is specified in nextflow.config, this directive allows overriding, many possibilties available, including local, sge, slurm, kubernetes. \n",
    "* ext\n",
    "* label - allows selection of processes, for e.g. to specify a set of resources for all processes with a given label\n",
    "* machineType\n",
    "* maxErrors\n",
    "* maxForks\n",
    "* maxRetries\n",
    "* memory\n",
    "* module - works with Environment Modules to load modules necessary for the process\n",
    "* penv\n",
    "* pod\n",
    "* publishDir - where the results are to be saved, e.g. `publishDir '/data/chunks', mode: 'copy', overwrite: false`\n",
    "* queue - which queue to use with sge\n",
    "* scratch\n",
    "* stageInMode\n",
    "* stageOutMode\n",
    "* storeDir\n",
    "* tag\n",
    "* time\n",
    "* validExitStatus\n",
    "\n",
    "There is an instruction to use the `tree` command to inspect the structure of the *work* directory, but that seems to be a bash command not installed on my laptop.\n",
    "\n",
    "There is also a `when` declaration that can be used to define a condition that must be verified in order to execute the process, e.g. \n",
    "```\n",
    "when:\n",
    "  proteins.name =~ /^BB11.*/ && type == 'nr'\n",
    "```\n",
    "This instruction is placed in the process block (as the `input`, `output` etc. sections) and it contains a condition. Pattern matching in `groovy` seems to be a rather lengthy topic. Took a while to figure out that to get expressions that **do not** match the pattern the unary negation operator needs to be used on the entire expression, e.g. \n",
    "```\n",
    "when:\n",
    "  !(proteins.name =~ /^BB11.*/ && type == 'nr')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f518bc8e",
   "metadata": {},
   "source": [
    "## script3.nf\n",
    "\n",
    "The main new concepts are introduced here:\n",
    "\n",
    "```\n",
    "/* read_pairs_ch = Channel .fromFilePairs(params.reads) */                                                             \n",
    "/* equivalent */                                                                                                       \n",
    "Channel.fromFilePairs(params.reads, checkIfExists: true).set { read_pairs_ch }                                         \n",
    "                                                                                                                       \n",
    "read_pairs_ch.view()                                                                                                   \n",
    "                                                                                                                       \n",
    "c = channel.from(['a','b','c'])                                                                                        \n",
    "c.view()                                                                                                               \n",
    "                                                                                                                       \n",
    "d = channel.fromPath(params.reads)                                                                                     \n",
    "d.view()                                                                                                               \n",
    "```\n",
    "\n",
    "The read files are defined like this:\n",
    "```\n",
    "params.reads = \"$baseDir/data/ggal/gut_{1,2}.fq\"\n",
    "```\n",
    "which results in two files matching the pattern. So the first line defines a *read_pairs_ch* initialized with pairs of files (here only one pair). The same initialization could be done as:\n",
    "```\n",
    "read_pairs_ch = Channel .fromFilePairs(params.reads)  \n",
    "\n",
    "read_pairs_ch.view()\n",
    "```\n",
    "The second line prints the content of the channel, i.e. the two file names.\n",
    "\n",
    "`fromFilePairs` is a function that should return tuples, in which the first element is the read pair prefix, and the second element the list of actual files. Why the prefix returned for this example is *gut* is not explained. It seems that it is found by removing the upstream directories as well as the underscore in the file name.\n",
    "\n",
    "Changing the specification of the reads files in the commandline like this:\n",
    "```\n",
    " ./nextflow run script3.nf --reads 'data/ggal/*_{1,2}.fq'\n",
    "``` \n",
    "leads to the wild card resolving to tuples of distinct prefixes (matched by the pattern, here <kbd>*</kbd>) and associated file lists. It does not result in a flat list of files.\n",
    "\n",
    "To get the code to produce an info message when no files are matching the provided pattern, one could use the following syntax (note the check, which may fail):\n",
    "```\n",
    "Channel.fromFilePairs(params.reads, checkIfExists: true).set { read_pairs_ch }\n",
    "```\n",
    "\n",
    "Channel creation can be done in other ways as well. There seem to be two types of channels:\n",
    "\n",
    "1. queue channel - unidirectional FIFO queue from which items are consumed\n",
    "2. value channel - single values that are not consumed, can be used by multiple processes\n",
    "\n",
    "Queue channels are created by channel factory methods (**Note** both *Channel.* and *channel.* can be used):\n",
    "* create - e.g. c = channel.create() \n",
    "* empty\n",
    "* from - e.g. c = channel.from( 1, 2, 3, 4 ) - creates a channel that emits the specified values (numbers/strings/lists - if a single list is specified view() outputs its elements, if multiple lists are specified view() outputs them as individual lists)\n",
    "* fromList - channel will emit elements of the list provided as argument\n",
    "* fromPath - e.g. d = channel.from(params.reads) - generates a list of all files that match the glob pattern specified in params.reads\n",
    "* fromFilePairs - demonstrated in the example\n",
    "* fromSRA - queries the NCBI SRA database and returns a channel emitting the fastq files matching the specified criteria i.e project or accession number(s).\n",
    "* value - creates a single value channel\n",
    "* watchPath - creates a channel that watches for modifications (default: creation) of files satisfying some name pattern \n",
    "\n",
    "**Note** white spaces between parentheses and argument or between arguments don't seem to make any difference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b83c84",
   "metadata": {},
   "source": [
    "## script4.nf\n",
    "\n",
    "Introduces a more complex process, depending on multiple inputs, in the quantification process. \n",
    "```\n",
    "process index {                                                                                                       \n",
    "\n",
    "    input:\n",
    "    path transcriptome from params.transcript\n",
    "    \n",
    "    output:\n",
    "    path 'index' into index_ch\n",
    "    \n",
    "    script:\n",
    "    \"\"\"\n",
    "    salmon index --threads $task.cpus -t $transcriptome -i index\n",
    "    \"\"\"                                                                                                               \n",
    "    \n",
    "}\n",
    "\n",
    "/*                                                                                                                     \n",
    " * Run Salmon to perform the quantification of expression using                                                       \n",
    " * the index and the matched read files                                                                               \n",
    " */                                                                                                                    \n",
    "process quantification {                                                                                               \n",
    "                                                                                                                       \n",
    "    publishDir \"QuantificationResults\"                                                                                 \n",
    "                                                                                                                       \n",
    "    input:                                                                                                             \n",
    "    path index from index_ch                                                                                           \n",
    "    tuple val(pair_id), path(reads) from read_pairs_ch                                                                 \n",
    "                                                                                                                       \n",
    "    output:                                                                                                           \n",
    "    path(pair_id) into quant_ch                                                                                                                                                                                                             \n",
    "    script:                                                                                                            \n",
    "    \"\"\"                                                                                                               \n",
    "    salmon quant --threads $task.cpus --libType=U -i $index -1 ${reads[0]} -2 ${reads[1]} -o $pair_id                 \n",
    "    \"\"\"                                                                                                               \n",
    "}\n",
    "```\n",
    "\n",
    "The first input is the index, which comes from the *index_ch* channel created by the previous process. A variable of type path, named **index** is created from the *index_ch* channel. Then, the tuple definition is used to extract tuples of values from the *read_pairs_ch*, which was created to contain a dictionary indexed by tissue with samples files as values. These values are cast to specific data types, `val` (scalar) for the tissue name and `path` for the sample file names. It seems that the path data type casting is broadcasted, as the *reads* variable created is an array.\n",
    "\n",
    "The output section specifies that and output channel *quant_ch* will be associated with a path variable containing the tissue name. This tissue name is used as file name in the salmon command as well. In this particular script, this channel is not used and the `output` declaration here seems superfluous (the workflow runs without it as well).\n",
    "\n",
    "Checked the `publishDir` directive, it creates a symlink with the specified name in the project directory, linking to the results directory of the current run. To actually copy the results in the specified directory the `mode: copy` option needs to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f15ec9",
   "metadata": {},
   "source": [
    "## script5.nf\n",
    "\n",
    "Adds another process, *fastqc*, with similar structure to *quantification*. \n",
    "```\n",
    "/* \n",
    " * Run fastQC to check quality of reads files \n",
    " */ \n",
    "\n",
    "process fastqc {\n",
    "    tag \"FASTQC on $sample_id\" \n",
    "    \n",
    "    input: \n",
    "    tuple val(sample_id), path(reads) from read_pairs_ch2 \n",
    "    \n",
    "    output:\n",
    "    path(\"fastqc_${sample_id}_logs\") into fastqc_ch \n",
    "    \n",
    "    script:\n",
    "    \"\"\"\n",
    "    mkdir fastqc_${sample_id}_logs\n",
    "    fastqc -o fastqc_${sample_id}_logs -f fastq -q ${reads}\n",
    "    \"\"\" \n",
    "}\n",
    "```\n",
    "However, it uses the same reads input channel, leading to error because the items from this channel are consumed by the *quantification* process. This can be fixed by making the fastqc script commands part of the script of the *quantification* process, that first uses the *read_pairs_ch* channel. However, the somewhat more elegant solution is to create a copy of the channel, changing the `set` for the `into` operator, and providing different names for the two channels. Syntax is important, `;` should be used between the two file names:\n",
    "```\n",
    "Channel                                                                                                                \n",
    "    .fromFilePairs( params.reads, checkIfExists:true )                                                                 \n",
    "    .into { read_pairs_ch; read_pairs_ch2 } \n",
    "```\n",
    "Finally, modules give an even more elegant solution to the problem, using function call-parameters type of logic for reusing channels (see below).\n",
    "\n",
    "Note also the `tag` directive that is used to give labels to executed processes, to make them easier identifiable in log files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65744df4",
   "metadata": {},
   "source": [
    "## script6.nf\n",
    "\n",
    "Adds the *multiqc* process. It uses the `publishDir` directive to save the output of the process into a directory specified in the parameter definition section. This is used as a root dir for the multiqc_report.html file created by the process. The process uses the `mix` and `collect` operators chained together to get all the outputs of the *quantification* and *fastqc* process as a single input.\n",
    "```\n",
    "/*\n",
    " * Create a report using multiQC for the quantification\n",
    " * and fastqc processes\n",
    " */\n",
    " \n",
    "process multiqc { \n",
    "\n",
    "    publishDir params.outdir, mode:'copy'\n",
    "    \n",
    "    input: \n",
    "    path('*') from quant_ch.mix(fastqc_ch).collect() \n",
    "    \n",
    "    output:\n",
    "    path('multiqc_report.html')\n",
    "    \n",
    "    script:\n",
    "    \"\"\" \n",
    "    multiqc .\n",
    "    \"\"\" \n",
    "}\n",
    "```\n",
    "\n",
    "quant_ch.mix(fastqx_ch) means mix the items of channel *fastqc_ch* into those of *quant_ch* (no specific order is guaranteed). *collect()* takes all the items, constructs and returns a list. This is necessary, because otherwise only one item is processed correctly. *multiqc* runs in a specified directory (here `.`) and collects all the data it can find in it. Not clear though how the items in the input channel feed into this directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721126c",
   "metadata": {},
   "source": [
    "## script7.nf\n",
    "\n",
    "Adds a task to be done on the completion of the workflow:\n",
    "```\n",
    "workflow.onComplete {                                                                     \n",
    "        log.info ( workflow.success ? \"\\nDone! Open the following report in your browser \\\n",
    "--> $params.outdir/multiqc_report.html\\n\" : \"Oops .. something went wrong\" )              \n",
    "}                                                                                         \n",
    "```                                                                                         \n",
    "This also shows the `C++`-like syntax of a shorthand if-else statement.\n",
    "\n",
    "If an email server is set up, an option can be used in the commandline of nextflow to send an email upon workflow completion (`-N email@address`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9beefe1",
   "metadata": {},
   "source": [
    "## Using custom scripts\n",
    "\n",
    "Custom scripts can be placed into a *bin* subdirectory of the project root directory. Then these scripts will be automatically added to the pipeline execution `PATH`. \n",
    "\n",
    "## Managing execution with executors\n",
    "\n",
    "Nextflow supports various systems for job distribution and execution such as the Sun Grid Engine (sge) and Slurm. There is also a `local` executor for local execution. We will most likely use Slurm. \n",
    "\n",
    "### Example specifying execution of the workflow using Slurm\n",
    "The `nextflow.config` file can be modified to include the following lines specifying relevant information for Slurm execution:\n",
    "\n",
    "```\n",
    "process.executor = 'slurm'\n",
    "process.queue = 'short'\n",
    "process.memory = '10 GB' \n",
    "process.time = '30 min'\n",
    "process.cpus = 4 \n",
    "```\n",
    "Equivalently, the definition of this data structure can be done using a `struct`/dictionary-like syntax:\n",
    "```\n",
    "process {\n",
    "    executor = 'slurm'\n",
    "    queue = 'short'\n",
    "    memory = '10 GB' \n",
    "    time = '30 min'\n",
    "    cpus = 4 \n",
    "```\n",
    "Process-specific configuration can be generated using selectors such as `withLabel` and `withName`, which will result in the application of the respective configuration only to processes with the specified label or name.\n",
    "\n",
    "More complex settings are also supported, e.g. \n",
    "```\n",
    "process {\n",
    "    withLabel: 'foo' { cpus = 2 }\n",
    "    withLabel: '!foo' { cpus = 4 }\n",
    "    withName: '!align.*' { queue = 'long' }\n",
    "}\n",
    "```\n",
    "Here the config for different types of processes is specified in a single definition, in which not only patterns but also their negation is allowed for process selection.\n",
    "\n",
    "Note that above we have set up configurations for *processes*. *Process* is just one of the `scopes` for which configurations can be defined. Many other `scopes` are available, see https://www.nextflow.io/docs/latest/config.html#config-scopes. \n",
    "\n",
    "Although in principle scope-specific configuration can be defined both in a config file and in the main nextflow script, I could not make this work for the `process` scope. For the `params` scope, I could only set it up in the config file. Including `process` scope definition in the main script gives a syntax error.\n",
    "\n",
    "The configuration file can also be organized in `profiles` that can be selectively used via a commandline parameter, `-profile standard` or `-profile cluster` assuming that the *standard* and *cluster* *profiles* were specified in the config file.\n",
    "\n",
    "### Workflows from github\n",
    "\n",
    "A workflow can be also pulled from github and run, e.g. the tutorial being located at https://github.com/nextflow-io/rnaseq-nf it can be run as follows:\n",
    "\n",
    "```\n",
    "./nextflow run nextflow-io/rnaseq-nf -with-docker\n",
    "```\n",
    "\n",
    "### Conda support\n",
    "\n",
    "Nextflow pipelines can automatically create and activate Conda environment. This can be done either on the command line: \n",
    "```\n",
    "./nextflow run script7.nf -with-conda env.yml\n",
    "```\n",
    "Or the Conda environment can be specified in the config file `process.conda = \"env.yml\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49094c4c",
   "metadata": {},
   "source": [
    "## Workflow modularization\n",
    "\n",
    "Processes and workflows can be combined hierarchically, similar to python functions and modules. A simple version of this approach, covered in the tutorial, is the following. \n",
    "\n",
    "There is a first-level file, `rnaseq-tasks.nf` which includes the definition of the processes. This is done pretty much as in the other scripts shown in the tutorial, except that the input sections of the processes are defined parametrically. That is, only the name of the parameters are specified, but not their bindings to various channels.\n",
    "\n",
    "The second-level script, `rnaseq-flow.nf`, includes the process definitions from `rnaseq-tasks.nf`, in a way similar to python module imports, e.g. \n",
    "```\n",
    "include { index; quantification; fastqc; multiqc  } from './rnaseq-tasks.nf'              \n",
    "```\n",
    "Then, the analysis steps are specified in terms of these functions:\n",
    "```\n",
    "workflow rnaseqFlow {                                                                     \n",
    "    // required inputs                                                                    \n",
    "    take:                                                                                 \n",
    "      transcriptome                                                                       \n",
    "      read_files                                                                          \n",
    "    // workflow implementation                                                            \n",
    "    main:                                                                                 \n",
    "      index(transcriptome)                                                                \n",
    "      quantification(index.out, read_files)                                               \n",
    "      fastqc(read_files)                                                                  \n",
    "      multiqc( quantification.out.mix(fastqc.out).collect() )                             \n",
    "}                                                                                         \n",
    "```\n",
    "There is a new concept, of a `workflow`, which takes some inputs and has a `main` section where the calls to the processes, with required parameters, are made. The workflow does not declare and it is implicitly to be the entry point of the application. Outputs of the workflow that should be visible in the outer scope can also be specified in an `emit` section, e.g.\n",
    "```\n",
    "workflow RNASEQ {\n",
    "  take:\n",
    "    transcriptome\n",
    "    read_pairs_ch\n",
    " \n",
    "  main: \n",
    "    INDEX(transcriptome)\n",
    "    FASTQC(read_pairs_ch)\n",
    "    QUANT(INDEX.out, read_pairs_ch)\n",
    "\n",
    "  emit: \n",
    "     QUANT.out.mix(FASTQC.out).collect()\n",
    "}\n",
    "```\n",
    "Some interesting things going on here: first, the parameters like read_files can be used multiple times, unlike channels which are consumed at their first use. This seems to be a feature added relatively recently to nextflow. Second, there output of a process can be accessed as <process_name>.out. Third, the outputs are treated as channels. These features come with the workflow definition functionality (DSL, currently DLS2 https://www.nextflow.io/blog/2020/dsl2-is-here.html). That's why the following command is included in the main script, which sets up the main inputs to the processes:\n",
    "```\n",
    "nextflow.enable.dsl=2\n",
    "\n",
    "/* \n",
    " * pipeline input parameters\n",
    " */\n",
    " \n",
    "params.reads = \"$baseDir/data/ggal/gut_{1,2}.fq\"\n",
    "params.transcript = \"$baseDir/data/ggal/transcriptome.fa\"\n",
    "params.multiqc = \"$baseDir/multiqc\"\n",
    "params.outdir = \"results\" \n",
    "\n",
    "log.info \"\"\"\\                                                                                                          \n",
    "         R N A S E Q - N F   P I P E L I N E                                                                           \n",
    "         ===================================                                                                           \n",
    "         transcriptome: ${params.transcript}                                                                           \n",
    "         reads        : ${params.reads}                                                                                \n",
    "         outdir       : ${params.outdir}                                                                               \n",
    "         \"\"\"                                                                                                           \n",
    "         .stripIndent() \n",
    "         \n",
    "include { rnaseqFlow } from './rnaseq-flow.nf'\n",
    "\n",
    "workflow {\n",
    "    read_pairs_ch = Channel .fromFilePairs( params.reads, checkIfExists:true )\n",
    "    rnaseqFlow( params.transcript, read_pairs_ch )\n",
    "}\n",
    "```\n",
    "\n",
    "Third, a wrapper script is used to define global parameters such as input file names and to set up the appropriate input channel. The call to the workflow is then made with the transcriptome file name and channel with read files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce05d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}